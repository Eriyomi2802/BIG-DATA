{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31effe58",
   "metadata": {},
   "source": [
    "# Tutoriel: Pyspark Intro - II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c1e9a3",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "papermill": {
     "duration": 0.043882,
     "end_time": "2022-08-05T19:27:29.592171",
     "exception": false,
     "start_time": "2022-08-05T19:27:29.548289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"0\"></a>\n",
    "# <p style=\"background-color:#00FF00;height: 60px;text-align: center;vertical-align: middle;line-height: 60px;;font-family:helvetica;color:#FFFFFF;font-size:120%;text-align:center;border-radius:12px 12px;\">Librairies </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5645260",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-05T19:27:29.681999Z",
     "iopub.status.busy": "2022-08-05T19:27:29.681128Z",
     "iopub.status.idle": "2022-08-05T19:27:29.692118Z",
     "shell.execute_reply": "2022-08-05T19:27:29.690980Z"
    },
    "papermill": {
     "duration": 0.061641,
     "end_time": "2022-08-05T19:27:29.695041",
     "exception": false,
     "start_time": "2022-08-05T19:27:29.633400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b79a540",
   "metadata": {
    "papermill": {
     "duration": 0.040133,
     "end_time": "2022-08-05T19:27:29.775813",
     "exception": false,
     "start_time": "2022-08-05T19:27:29.735680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "<p style = \"font-size:140%\"> Ceci est un tutoriel d'introduction à Pyspark. Vous y trouverez quelques concepts de base et principales fonctions de Pyspark.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da44f445",
   "metadata": {
    "papermill": {
     "duration": 0.040193,
     "end_time": "2022-08-05T19:27:29.856857",
     "exception": false,
     "start_time": "2022-08-05T19:27:29.816664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# CONTENT TABLE\n",
    "----------------\n",
    "## Le concept\n",
    "* [1) Qu'est-ce que Pyspark](#01)\n",
    "  \n",
    "\n",
    "## La pratique\n",
    "* [1) GETTING RUNNING ON NOTEBOOKS](#1)\n",
    "* [2) READING DATA FROM DIFFERENT SOURCES](#2)\n",
    "* [3) AFFICHAGE](#3)\n",
    "* [4) FILTRAGE](#4)\n",
    "* [5) FILTRAGE PARTIE 2](#5)\n",
    "* [6) MANIPULATION DE DONNEES](#6)\n",
    "* [7) GROUPING BY](#7)\n",
    "* [8) WINDOW FUNCTION](#8)\n",
    "* [9) JOIN](#9)\n",
    "* [10) UDF](#10)\n",
    "* [11) REFERENCES](#20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdc95e9",
   "metadata": {
    "papermill": {
     "duration": 0.042914,
     "end_time": "2022-08-05T19:27:29.941506",
     "exception": false,
     "start_time": "2022-08-05T19:27:29.898592",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"01\"></a>\n",
    "# <p style=\"background-color:#00FF00;height: 60px;text-align: center;vertical-align: middle;line-height: 60px;;font-family:helvetica;color:#FFFFFF;font-size:120%;text-align:center;border-radius:12px 12px;\"> Qu'est-ce que Pyspark </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ddb8ac",
   "metadata": {
    "papermill": {
     "duration": 0.039743,
     "end_time": "2022-08-05T19:27:30.021414",
     "exception": false,
     "start_time": "2022-08-05T19:27:29.981671",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<p style = \"font-size:120%\"> \n",
    "    Apache Spark is an open source unified computing engine for distributed data processing on computer clusters, allowing an easy and scalable developments. It supports many programing languages such as Python, R and Scala. Also, includes libraries from SQL and Machine Learning. The structure is designed to deal with huge amount of data distributing large datasets among \"computers\". Each one of the computers os called a \"Executors\" and the managemnt of all data is done by a so called \"Driver\".<br>\n",
    "    <br>\n",
    "    Pyspark is a library for python that enables to run python applications in the Apache Spark architeture, hence, allowing parallel distribuiton to cope with large data \n",
    "\n",
    "</p>\n",
    "    \n",
    "<img src=\"https://raw.githubusercontent.com/paulojunqueira/files/main/Sem%20t%C3%ADtulo.gif\" style = 'class=center; margin-left: auto; margin-right: auto;width: 50%;display: block;'>  \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077948b2",
   "metadata": {
    "papermill": {
     "duration": 0.040027,
     "end_time": "2022-08-05T19:27:30.181939",
     "exception": false,
     "start_time": "2022-08-05T19:27:30.141912",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ed67ed",
   "metadata": {
    "papermill": {
     "duration": 0.04073,
     "end_time": "2022-08-05T19:27:30.264251",
     "exception": false,
     "start_time": "2022-08-05T19:27:30.223521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "# <p style=\"background-color:#00FF00;height: 60px;text-align: center;vertical-align: middle;line-height: 60px;;font-family:helvetica;color:#FFFFFF;font-size:120%;text-align:center;border-radius:12px 12px;\">GETTING RUNNING ON NOTEBOOKS</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ff51b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:27:30.350195Z",
     "iopub.status.busy": "2022-08-05T19:27:30.349066Z",
     "iopub.status.idle": "2022-08-05T19:28:24.385251Z",
     "shell.execute_reply": "2022-08-05T19:28:24.384202Z"
    },
    "papermill": {
     "duration": 54.082907,
     "end_time": "2022-08-05T19:28:24.389138",
     "exception": false,
     "start_time": "2022-08-05T19:27:30.306231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install pyspark -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3831494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 12\n"
     ]
    }
   ],
   "source": [
    "# Get the number of CPU cores\n",
    "num_cores = os.cpu_count()\n",
    "print(\"Number of CPU cores:\", num_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "858d0bd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:24.486672Z",
     "iopub.status.busy": "2022-08-05T19:28:24.486308Z",
     "iopub.status.idle": "2022-08-05T19:28:30.524191Z",
     "shell.execute_reply": "2022-08-05T19:28:30.523199Z"
    },
    "papermill": {
     "duration": 6.09017,
     "end_time": "2022-08-05T19:28:30.527450",
     "exception": false,
     "start_time": "2022-08-05T19:28:24.437280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To run pypsark in notebooks: \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Import libraries to create a dataframe\n",
    "#It is also common to import as T: #from pypsark.sql.types as T\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e71a3f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://W0010SL00066699.green.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f3e275a250>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#by default appName is set to: pyspark-shell\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b2cac0",
   "metadata": {
    "papermill": {
     "duration": 0.041673,
     "end_time": "2022-08-05T19:28:30.612724",
     "exception": false,
     "start_time": "2022-08-05T19:28:30.571051",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"2\"></a>\n",
    "# <p style=\"background-color:#00FF00;height: 60px;text-align: center;vertical-align: middle;line-height: 60px;;font-family:helvetica;color:#FFFFFF;font-size:120%;text-align:center;border-radius:12px 12px;\"> READING DATA FROM DIFFERENT SOURCES</p>\n",
    "\n",
    "<li style = \"font-size:110%\">Create a dataframe in Pyspark\n",
    "<li style = \"font-size:110%\">Read a csv in Pyspark\n",
    "<li style = \"font-size:110%\">Convert a Pandas df to Pyspark df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b84363e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:30.699235Z",
     "iopub.status.busy": "2022-08-05T19:28:30.698410Z",
     "iopub.status.idle": "2022-08-05T19:28:38.056588Z",
     "shell.execute_reply": "2022-08-05T19:28:38.055799Z"
    },
    "papermill": {
     "duration": 7.404959,
     "end_time": "2022-08-05T19:28:38.059401",
     "exception": false,
     "start_time": "2022-08-05T19:28:30.654442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o87.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (W0010SL00066699.green.local executor driver): java.io.IOException: Cannot run program \"C:\\Users\\PMA831\\AppData\\Local\\Programs\\Python\\Python38\": CreateProcess error=5, Accès refusé\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.IOException: CreateProcess error=5, Accès refusé\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:487)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:154)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 31 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.IOException: Cannot run program \"C:\\Users\\PMA831\\AppData\\Local\\Programs\\Python\\Python38\": CreateProcess error=5, Accès refusé\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=5, Accès refusé\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:487)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:154)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 31 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m df_example \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(example_data, schema \u001b[38;5;241m=\u001b[39m schema)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#displaying data \u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mdf_example\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PMA831\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\sql\\dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\PMA831\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\PMA831\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\PMA831\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o87.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5) (W0010SL00066699.green.local executor driver): java.io.IOException: Cannot run program \"C:\\Users\\PMA831\\AppData\\Local\\Programs\\Python\\Python38\": CreateProcess error=5, Accès refusé\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.IOException: CreateProcess error=5, Accès refusé\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:487)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:154)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 31 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.IOException: Cannot run program \"C:\\Users\\PMA831\\AppData\\Local\\Programs\\Python\\Python38\": CreateProcess error=5, Accès refusé\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=5, Accès refusé\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:487)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:154)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 31 more\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Toy dataset for example\n",
    "example_data = [(100,\"Brazil\",\"1000\",\"A100\",6),\n",
    "    (101,\"Spain\",\"2000\",\"MA100\",2),\n",
    "    (102,\"EUA\",\"3000\",\"A200\",10),\n",
    "    (110,\"Mexico\",\"B100\",\"F400\",8),\n",
    "    (200,\"Japan\",\"5000\",\"A100\",9),\n",
    "    (880,\"EUA\",\"500\",\"Z120\",1)\n",
    "  ]\n",
    "\n",
    "#Defining a Schema of the Dataset (name, type of column and if its nullable)\n",
    "schema = StructType([StructField('key', IntegerType(), True),\n",
    "                    StructField('C1', StringType(), True),\n",
    "                    StructField('C2', StringType(), True),\n",
    "                    StructField('C3', StringType(), True),\n",
    "                    StructField('C4', IntegerType(), True)])\n",
    "\n",
    "df_example = spark.createDataFrame(example_data, schema = schema)\n",
    "\n",
    "#displaying data \n",
    "df_example.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfc5a5f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:38.176864Z",
     "iopub.status.busy": "2022-08-05T19:28:38.176256Z",
     "iopub.status.idle": "2022-08-05T19:28:38.621924Z",
     "shell.execute_reply": "2022-08-05T19:28:38.620656Z"
    },
    "papermill": {
     "duration": 0.523635,
     "end_time": "2022-08-05T19:28:38.626012",
     "exception": false,
     "start_time": "2022-08-05T19:28:38.102377",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o62.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (W0010SL00066699.green.local executor driver): java.io.IOException: Cannot run program \"C:\\Users\\PMA831\\AppData\\Local\\Programs\\Python\\Python38\": CreateProcess error=5, Accès refusé\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.IOException: CreateProcess error=5, Accès refusé\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:487)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:154)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 31 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.IOException: Cannot run program \"C:\\Users\\PMA831\\AppData\\Local\\Programs\\Python\\Python38\": CreateProcess error=5, Accès refusé\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=5, Accès refusé\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:487)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:154)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 31 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m df_example_2 \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(example_data_2, schema \u001b[38;5;241m=\u001b[39m schema_2)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#displaying data \u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mdf_example_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PMA831\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\sql\\dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\PMA831\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\PMA831\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\PMA831\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o62.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (W0010SL00066699.green.local executor driver): java.io.IOException: Cannot run program \"C:\\Users\\PMA831\\AppData\\Local\\Programs\\Python\\Python38\": CreateProcess error=5, Accès refusé\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.IOException: CreateProcess error=5, Accès refusé\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:487)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:154)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 31 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.IOException: Cannot run program \"C:\\Users\\PMA831\\AppData\\Local\\Programs\\Python\\Python38\": CreateProcess error=5, Accès refusé\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:167)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.io.IOException: CreateProcess error=5, Accès refusé\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:487)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:154)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 31 more\r\n"
     ]
    }
   ],
   "source": [
    "#Toy dataset for example\n",
    "example_data_2 = [(100,\"BB\",10),\n",
    "    (101,\"XX\",99),\n",
    "    (102,\"AN\",898),\n",
    "    (110,\"AC\",567),\n",
    "    (200,\"AV\",344),\n",
    "    (300,\"FV\",834),\n",
    "    (111,\"ZW\",54)\n",
    "  ]\n",
    "\n",
    "\n",
    "#Defining a Schema of the Dataset (name, type of column and if its nullable)\n",
    "schema_2 = StructType([StructField('key', IntegerType(), True),\n",
    "                    StructField('C5', StringType(), True),\n",
    "                    StructField('C6', IntegerType(), True)])\n",
    "\n",
    "df_example_2 = spark.createDataFrame(example_data_2, schema = schema_2)\n",
    "\n",
    "#displaying data \n",
    "df_example_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66734956",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:38.719517Z",
     "iopub.status.busy": "2022-08-05T19:28:38.719159Z",
     "iopub.status.idle": "2022-08-05T19:28:40.597014Z",
     "shell.execute_reply": "2022-08-05T19:28:40.594513Z"
    },
    "papermill": {
     "duration": 1.928114,
     "end_time": "2022-08-05T19:28:40.602599",
     "exception": false,
     "start_time": "2022-08-05T19:28:38.674485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) reading a csv in spark\n",
    "df_t = spark.read.csv('input/titanic/train.csv', header = True, inferSchema = True)\n",
    "\n",
    "#displaying data\n",
    "df_t.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4552545d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:40.740292Z",
     "iopub.status.busy": "2022-08-05T19:28:40.739294Z",
     "iopub.status.idle": "2022-08-05T19:28:41.461141Z",
     "shell.execute_reply": "2022-08-05T19:28:41.457848Z"
    },
    "papermill": {
     "duration": 0.793854,
     "end_time": "2022-08-05T19:28:41.464716",
     "exception": false,
     "start_time": "2022-08-05T19:28:40.670862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3) Read a pandas dataframe \n",
    "df_pandas = pd.read_csv('input/wine-quality-dataset/WineQT.csv')\n",
    "\n",
    "#Converting it to a a spark dataframe\n",
    "df = spark.createDataFrame(df_pandas)\n",
    "\n",
    "#Going back to pandas (warning, for huge datasets in distributed systems this command may cause memory overload)\n",
    "# df.toPandas()\n",
    "\n",
    "#displaying data\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf2e4f",
   "metadata": {
    "papermill": {
     "duration": 0.056961,
     "end_time": "2022-08-05T19:28:41.599247",
     "exception": false,
     "start_time": "2022-08-05T19:28:41.542286",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"3\"></a>\n",
    "# <p style=\"background-color:#00FF00;height: 60px;text-align: center;vertical-align: middle;line-height: 60px;;font-family:helvetica;color:#FFFFFF;font-size:120%;text-align:center;border-radius:12px 12px;\"> AFFICHAGE</p>\n",
    "\n",
    "<li style = \"font-size:110%\"> Displaying table\n",
    "<li style = \"font-size:110%\"> Analysing statistics and types\n",
    "<li style = \"font-size:110%\"> Selecting variables\n",
    "<li style = \"font-size:110%\"> Droping Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6fb66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:41.695778Z",
     "iopub.status.busy": "2022-08-05T19:28:41.695286Z",
     "iopub.status.idle": "2022-08-05T19:28:42.039825Z",
     "shell.execute_reply": "2022-08-05T19:28:42.038734Z"
    },
    "papermill": {
     "duration": 0.399051,
     "end_time": "2022-08-05T19:28:42.043249",
     "exception": false,
     "start_time": "2022-08-05T19:28:41.644198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As Already seen, showing the results. Also could use a parameter to change the number of rows displayed\n",
    "print('Example_1')\n",
    "df.show(10)\n",
    "\n",
    "print('Example_2')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce025fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:42.138450Z",
     "iopub.status.busy": "2022-08-05T19:28:42.138077Z",
     "iopub.status.idle": "2022-08-05T19:28:42.893112Z",
     "shell.execute_reply": "2022-08-05T19:28:42.891415Z"
    },
    "papermill": {
     "duration": 0.806193,
     "end_time": "2022-08-05T19:28:42.896848",
     "exception": false,
     "start_time": "2022-08-05T19:28:42.090655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use limit() to determine the number of lines to show as well\n",
    "df.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94badf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:42.993968Z",
     "iopub.status.busy": "2022-08-05T19:28:42.993633Z",
     "iopub.status.idle": "2022-08-05T19:28:43.007410Z",
     "shell.execute_reply": "2022-08-05T19:28:43.005613Z"
    },
    "papermill": {
     "duration": 0.064995,
     "end_time": "2022-08-05T19:28:43.010221",
     "exception": false,
     "start_time": "2022-08-05T19:28:42.945226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Checking schema of df ( types of variables )\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5c2e19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:43.111761Z",
     "iopub.status.busy": "2022-08-05T19:28:43.111067Z",
     "iopub.status.idle": "2022-08-05T19:28:43.120448Z",
     "shell.execute_reply": "2022-08-05T19:28:43.119365Z"
    },
    "papermill": {
     "duration": 0.063256,
     "end_time": "2022-08-05T19:28:43.123670",
     "exception": false,
     "start_time": "2022-08-05T19:28:43.060414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking Columns' name\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a060602",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:43.220876Z",
     "iopub.status.busy": "2022-08-05T19:28:43.220127Z",
     "iopub.status.idle": "2022-08-05T19:28:44.000877Z",
     "shell.execute_reply": "2022-08-05T19:28:43.999807Z"
    },
    "papermill": {
     "duration": 0.833002,
     "end_time": "2022-08-05T19:28:44.004792",
     "exception": false,
     "start_time": "2022-08-05T19:28:43.171790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking statistics of these variables\n",
    "df.select('fixed acidity', 'quality').describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4470a70c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:44.124315Z",
     "iopub.status.busy": "2022-08-05T19:28:44.123994Z",
     "iopub.status.idle": "2022-08-05T19:28:44.497793Z",
     "shell.execute_reply": "2022-08-05T19:28:44.495308Z"
    },
    "papermill": {
     "duration": 0.431479,
     "end_time": "2022-08-05T19:28:44.503504",
     "exception": false,
     "start_time": "2022-08-05T19:28:44.072025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Counting the number of rows (samples)\n",
    "print('Number of rows: ',df.count())\n",
    "\n",
    "#Counting the number of columns (variables)\n",
    "print('Number of Columns: ', len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c36c19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:44.629990Z",
     "iopub.status.busy": "2022-08-05T19:28:44.629655Z",
     "iopub.status.idle": "2022-08-05T19:28:45.864310Z",
     "shell.execute_reply": "2022-08-05T19:28:45.862898Z"
    },
    "papermill": {
     "duration": 1.291554,
     "end_time": "2022-08-05T19:28:45.868350",
     "exception": false,
     "start_time": "2022-08-05T19:28:44.576796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Cross table between variables\n",
    "df.crosstab('pH', 'quality').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbce215",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:46.014679Z",
     "iopub.status.busy": "2022-08-05T19:28:46.014202Z",
     "iopub.status.idle": "2022-08-05T19:28:46.192882Z",
     "shell.execute_reply": "2022-08-05T19:28:46.191831Z"
    },
    "papermill": {
     "duration": 0.254169,
     "end_time": "2022-08-05T19:28:46.196987",
     "exception": false,
     "start_time": "2022-08-05T19:28:45.942818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Selecting specific columns\n",
    "df.select('fixed acidity', 'quality').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0461e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:46.339890Z",
     "iopub.status.busy": "2022-08-05T19:28:46.339411Z",
     "iopub.status.idle": "2022-08-05T19:28:46.975469Z",
     "shell.execute_reply": "2022-08-05T19:28:46.974367Z"
    },
    "papermill": {
     "duration": 0.712328,
     "end_time": "2022-08-05T19:28:46.978960",
     "exception": false,
     "start_time": "2022-08-05T19:28:46.266632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Ordering by a column\n",
    "df.orderBy('quality', ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e151c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordering by a column\n",
    "df.orderBy('quality', ascending = True).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9bb656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting & ordering \n",
    "df.select('fixed acidity', 'quality').orderBy('quality', ascending = False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c3686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:47.113468Z",
     "iopub.status.busy": "2022-08-05T19:28:47.112997Z",
     "iopub.status.idle": "2022-08-05T19:28:47.299324Z",
     "shell.execute_reply": "2022-08-05T19:28:47.298306Z"
    },
    "papermill": {
     "duration": 0.249195,
     "end_time": "2022-08-05T19:28:47.304602",
     "exception": false,
     "start_time": "2022-08-05T19:28:47.055407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Removing variables (columns)\n",
    "df.drop('fixed acidity','volatile acidity').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d8ced1",
   "metadata": {
    "papermill": {
     "duration": 0.05128,
     "end_time": "2022-08-05T19:28:47.409375",
     "exception": false,
     "start_time": "2022-08-05T19:28:47.358095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"4\"></a>\n",
    "# <p style=\"background-color:#00FF00;height: 60px;text-align: center;vertical-align: middle;line-height: 60px;;font-family:helvetica;color:#FFFFFF;font-size:120%;text-align:center;border-radius:12px 12px;\"> FILTRAGE DE DONNEES - PARTIE 1</p>\n",
    "<li style = \"font-size:110%\">Pour filtrer les données en fonction de certaines conditions, nous allons utiliser les fonctions de la librairie SQL.   \n",
    "<li style = \"font-size:110%\">sql.functions va nous permettre d'appeler et d'utiliser des fonctions sql très utiles pour le traitement et la manipulation des données. Pour en savoir plus:\n",
    "    \n",
    "[SQL Functions in Spark](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.functions.arrays_zip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef62e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73fc0f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:47.512494Z",
     "iopub.status.busy": "2022-08-05T19:28:47.512162Z",
     "iopub.status.idle": "2022-08-05T19:28:48.372027Z",
     "shell.execute_reply": "2022-08-05T19:28:48.370998Z"
    },
    "papermill": {
     "duration": 0.915771,
     "end_time": "2022-08-05T19:28:48.375621",
     "exception": false,
     "start_time": "2022-08-05T19:28:47.459850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#filtering Data\n",
    "print('---- Example_1 ----')\n",
    "df.filter(F.col('quality')>5).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61bca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering Data\n",
    "print('---- Example_2 ----')\n",
    "df.filter(F.col('fixed acidity')< 5).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d1a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering Data\n",
    "print('---- Example_3 Multiple conditions ----')\n",
    "df.filter((F.col('fixed acidity') < 10) & (F.col('quality') > 5)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e914047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering Data\n",
    "print('---- Example_4 Multiple conditions ----')\n",
    "df.filter((F.col('quality') > 4) | (F.col('quality') < 8)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d49b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering Data\n",
    "print('Example_5 in list')\n",
    "df.filter((F.col('quality').isin([5,6,7]))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fcd961",
   "metadata": {
    "papermill": {
     "duration": 0.055217,
     "end_time": "2022-08-05T19:28:48.483934",
     "exception": false,
     "start_time": "2022-08-05T19:28:48.428717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"5\"></a>\n",
    "# <p style=\"background-color:#00FF00;height: 60px;text-align: center;vertical-align: middle;line-height: 60px;;font-family:helvetica;color:#FFFFFF;font-size:120%;text-align:center;border-radius:12px 12px;\"> FILTRAGE DE DONNEES - PARTIE 2</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bd14f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:48.597224Z",
     "iopub.status.busy": "2022-08-05T19:28:48.596823Z",
     "iopub.status.idle": "2022-08-05T19:28:50.598143Z",
     "shell.execute_reply": "2022-08-05T19:28:50.595848Z"
    },
    "papermill": {
     "duration": 2.060507,
     "end_time": "2022-08-05T19:28:50.602616",
     "exception": false,
     "start_time": "2022-08-05T19:28:48.542109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Part 2\n",
    "\n",
    "#filtering Data\n",
    "print('---- Example_6 Categorical ----')\n",
    "df_t.filter((F.col('Embarked') ==  'S') & (F.col('Sex') ==  'female')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae48df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering Data\n",
    "print('---- Example_7 Exclusion ----')\n",
    "df_t.filter(~(F.col('Embarked') ==  'S')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2eed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering Data\n",
    "print('---- Example_8 Exclusion SQL  ----')\n",
    "df_t.filter(('Embarked !=  \"C\"')).show(5)\n",
    "df_t.filter(('Embarked <>  \"Q\"')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4debd2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering Data\n",
    "print('---- Example_9 Cabin Starts with A----')\n",
    "df_t.filter((F.col('Cabin').startswith('A'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbb7b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering Data\n",
    "print('---- Example_10 Cabin Ends with 3 ----')\n",
    "df_t.filter((F.col('Cabin').endswith('3'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering Data\n",
    "print('---- Example_11 Name Contains \"Miss\" ----')\n",
    "df_t.filter((F.col('Name').contains('Miss'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb1cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering Data\n",
    "print('---- Example_12 like (REGEX)----')\n",
    "df_t.filter((F.col('Name').like('%John%'))).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3649206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering Data\n",
    "print('---- Example_13 Cabin null----')\n",
    "df_t.filter((F.col('Cabin').isNull())).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c49e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering Data\n",
    "print('---- Example_14 Cabin not null----')\n",
    "df_t.filter((F.col('Cabin').isNotNull())).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0b878",
   "metadata": {
    "papermill": {
     "duration": 0.054523,
     "end_time": "2022-08-05T19:28:50.729947",
     "exception": false,
     "start_time": "2022-08-05T19:28:50.675424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"6\"></a>\n",
    "# <p style=\"background-color:#00FF00;height: 60px;text-align: center;vertical-align: middle;line-height: 60px;;font-family:helvetica;color:#FFFFFF;font-size:120%;text-align:center;border-radius:12px 12px;\"> MANIPULATION DE DONNEES</p>\n",
    "<li style = \"font-size:110%\"> Create new columns \n",
    "<li style = \"font-size:110%\"> Changing columns \n",
    "\n",
    "withColumn() F.round()  F.when() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda8202",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:50.840202Z",
     "iopub.status.busy": "2022-08-05T19:28:50.839870Z",
     "iopub.status.idle": "2022-08-05T19:28:51.316093Z",
     "shell.execute_reply": "2022-08-05T19:28:51.313996Z"
    },
    "papermill": {
     "duration": 0.536054,
     "end_time": "2022-08-05T19:28:51.320654",
     "exception": false,
     "start_time": "2022-08-05T19:28:50.784600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Manipulating Data\n",
    "print('---- Example_1 Adding up two Columns----') #Same goes to all math operators\n",
    "df.withColumn('VOLATILE + CITRIC', F.col('volatile acidity') + F.col('citric acid')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cd26b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manipulating Data\n",
    "print('---- Example_2 Create a column of ones----') \n",
    "df.withColumn('DUMMY_ONES', F.lit(1)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2f902",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manipulating Data\n",
    "print('---- Example_3 round pH variable----') \n",
    "df.withColumn('pH_ROUNDED', F.round('pH',1)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4942c8",
   "metadata": {
    "papermill": {
     "duration": 0.081938,
     "end_time": "2022-08-05T19:28:51.492963",
     "exception": false,
     "start_time": "2022-08-05T19:28:51.411025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Conditionals (F.when('Condition', value ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588bfa8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:51.653432Z",
     "iopub.status.busy": "2022-08-05T19:28:51.653013Z",
     "iopub.status.idle": "2022-08-05T19:28:52.128662Z",
     "shell.execute_reply": "2022-08-05T19:28:52.127551Z"
    },
    "papermill": {
     "duration": 0.562027,
     "end_time": "2022-08-05T19:28:52.134676",
     "exception": false,
     "start_time": "2022-08-05T19:28:51.572649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Manipulating Data\n",
    "\n",
    "print('---- Example_1 Conditional based on Survived Column----') \n",
    "df_t.withColumn('Survived_TEXT', F.when(F.col('Survived') == 0, 'NO')\\\n",
    "                                 .otherwise('YES')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022546ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---- Example_2 Multiple Conditions----') \n",
    "df_t.withColumn('COD_EMBARKED', F.when(F.col('Embarked') == 'C', 'YES_C')\\\n",
    "                                 .when(F.col('Embarked') == 'S', 'NO_S')\\\n",
    "                                 .otherwise('MAYBE_OTHER')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e32842",
   "metadata": {
    "papermill": {
     "duration": 0.053383,
     "end_time": "2022-08-05T19:28:52.245124",
     "exception": false,
     "start_time": "2022-08-05T19:28:52.191741",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<li style = \"font-size:110%\">Changing type of variables (importing the sql types module as T)\n",
    "<li style = \"font-size:110%\">DoubleType, IntegerType, StringType..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f91b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pyspark.sql.types as T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f98325",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:52.355189Z",
     "iopub.status.busy": "2022-08-05T19:28:52.354829Z",
     "iopub.status.idle": "2022-08-05T19:28:52.656349Z",
     "shell.execute_reply": "2022-08-05T19:28:52.654127Z"
    },
    "papermill": {
     "duration": 0.361042,
     "end_time": "2022-08-05T19:28:52.660713",
     "exception": false,
     "start_time": "2022-08-05T19:28:52.299671",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Manipulating Data\n",
    "print('Original Schema')\n",
    "df_t.select('Survived','Age').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568cb7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manipulating Data\n",
    "print('---- Example_1 Survid to type Double----') \n",
    "df_t_1 = df_t.withColumn('Survived', F.col('Survived').cast(T.DoubleType()))\n",
    "df_t_1.show(5)\n",
    "print('New Survived Schema')\n",
    "df_t_1.select('Survived').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd133a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manipulating Data\n",
    "print('---- Example_2 Age to type String ----') \n",
    "df_t_1 = df_t.withColumn('Age', F.col('Age').cast(T.StringType()))\n",
    "df_t_1.show(5)\n",
    "print('New Age Schema')\n",
    "df_t_1.select('Age').printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a411e2",
   "metadata": {
    "papermill": {
     "duration": 0.057298,
     "end_time": "2022-08-05T19:28:52.776225",
     "exception": false,
     "start_time": "2022-08-05T19:28:52.718927",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"7\"></a>\n",
    "# <p style=\"background-color:#00FF00;height: 60px;text-align: center;vertical-align: middle;line-height: 60px;;font-family:helvetica;color:#FFFFFF;font-size:120%;text-align:center;border-radius:12px 12px;\"> GROUPING BY</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12501ee1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:52.891032Z",
     "iopub.status.busy": "2022-08-05T19:28:52.890696Z",
     "iopub.status.idle": "2022-08-05T19:28:55.915731Z",
     "shell.execute_reply": "2022-08-05T19:28:55.914886Z"
    },
    "papermill": {
     "duration": 3.086401,
     "end_time": "2022-08-05T19:28:55.919751",
     "exception": false,
     "start_time": "2022-08-05T19:28:52.833350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Group By\n",
    "print('---- Example_1 ---- Grouping by Sex and counting the total samples of each Genre')\n",
    "df_t.groupBy('Sex').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c1d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group By\n",
    "print('---- Example_2 ---- Grouping by Survived and counting the total samples of each class')\n",
    "df_t.groupBy('Survived').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16742cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group By\n",
    "print('---- Example_3 ---- Grouping by Sex and survived and counting the total samples of each class')\n",
    "df_t.groupBy('Survived', 'Sex').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97821062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group By\n",
    "print('---- Example_3 ---- Grouping by Survived and get the age mean (avg)')\n",
    "df_t.groupBy('Survived').agg(F.mean(F.col('Age'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2005ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group By\n",
    "print('---- Example_3.1 ---- Grouping by Survived and get the age mean (avg) and Std. Also(Changing the column´s name)')\n",
    "df_t.groupBy('Survived').agg(F.mean(F.col('Age')).alias('Mean Age'),\n",
    "                             F.stddev(F.col('Age')).alias('Std Age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd6f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group By\n",
    "print('---- Example_4 ---- Grouping by quality')\n",
    "df.groupBy('quality').agg(F.mean(F.col('fixed acidity')).alias('fixed acidity mean'),\n",
    "                         F.stddev(F.col('fixed acidity')).alias('Std fixed acidity'),\n",
    "                         F.mean(F.col('residual sugar')).alias('residual sugar mean'),\n",
    "                         F.stddev(F.col('residual sugar')).alias('Std residual sugar')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd81227b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group By\n",
    "print('---- Example_5 ---- Grouping  age and counting only age > 20')\n",
    "df_t.groupBy('Age').agg(F.when(F.col('Age') > 20, \\\n",
    "                            F.count(F.col('Age'))).alias('Count Age > 20')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb54d1",
   "metadata": {
    "papermill": {
     "duration": 0.090772,
     "end_time": "2022-08-05T19:28:56.100261",
     "exception": false,
     "start_time": "2022-08-05T19:28:56.009489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"8\"></a>\n",
    "# <p style=\"background-color:#00FF00;height: 60px;text-align: center;vertical-align: middle;line-height: 60px;;font-family:helvetica;color:#FFFFFF;font-size:120%;text-align:center;border-radius:12px 12px;\"> WINDOW FUNCTION</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1f14c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:56.224061Z",
     "iopub.status.busy": "2022-08-05T19:28:56.223708Z",
     "iopub.status.idle": "2022-08-05T19:28:58.041080Z",
     "shell.execute_reply": "2022-08-05T19:28:58.039942Z"
    },
    "papermill": {
     "duration": 1.885034,
     "end_time": "2022-08-05T19:28:58.044967",
     "exception": false,
     "start_time": "2022-08-05T19:28:56.159933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e93b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a sequencial column in the time window\n",
    "print('---- Example_1 ---- Creating a sequential Row')\n",
    "window  = Window.partitionBy(\"quality\").orderBy(\"quality\")\n",
    "\n",
    "df_temp = df.withColumn(\"ROW COLUMN\",F.row_number().over(window))\n",
    "df_temp.select('quality','alcohol','ROW COLUMN').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83fc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#suming alcohol values over the quality variable\n",
    "print('---- Example_2 ---- Sum over sequence')\n",
    "window  = Window.partitionBy(\"quality\").orderBy(\"quality\")\n",
    "\n",
    "df_temp = df.withColumn(\"sum\",F.sum(F.col('alcohol')).over(window))\n",
    "df_temp.select('quality','alcohol','sum').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cumulative sum over alcohol by quality\n",
    "print('---- Example_3 ---- Cumulative Sum over alcohol with ROUND')\n",
    "window  = Window.partitionBy(\"quality\").orderBy(\"alcohol\").rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "df_temp = df.withColumn(\"CUMULATIVE_alcohol\",F.round(F.sum(F.col('alcohol')).over(window),1))\n",
    "df_temp.select('quality','alcohol','CUMULATIVE_alcohol').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Laging\n",
    "print('---- Example_5 ---- Laging alcohol varibale by 2 (moving down 2 positions)')\n",
    "window  = Window.partitionBy(\"quality\").orderBy(\"alcohol\")\n",
    "\n",
    "df_temp = df.withColumn(\"Lagged_alcohol\",F.lag(F.col('alcohol'),2).over(window))\n",
    "df_temp.select('quality','alcohol','Lagged_alcohol').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9dc8cd",
   "metadata": {
    "papermill": {
     "duration": 0.082921,
     "end_time": "2022-08-05T19:28:58.219899",
     "exception": false,
     "start_time": "2022-08-05T19:28:58.136978",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"9\"></a>\n",
    "# <p style=\"background-color:#00FF00;height: 60px;text-align: center;vertical-align: middle;line-height: 60px;;font-family:helvetica;color:#FFFFFF;font-size:120%;text-align:center;border-radius:12px 12px;\"> JOIN </p>\n",
    "\n",
    "<li style = \"font-size:110%\"> Join\n",
    "<li style = \"font-size:110%\"> unionByName\n",
    "<li style = \"font-size:110%\"> union \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65967e6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:58.343601Z",
     "iopub.status.busy": "2022-08-05T19:28:58.343224Z",
     "iopub.status.idle": "2022-08-05T19:28:59.709942Z",
     "shell.execute_reply": "2022-08-05T19:28:59.707715Z"
    },
    "papermill": {
     "duration": 1.433905,
     "end_time": "2022-08-05T19:28:59.714637",
     "exception": false,
     "start_time": "2022-08-05T19:28:58.280732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('---- Example_1 ---- Join with a inner (If the keys don´t match,the value is droped)')\n",
    "df_merged = df_example.join(df_example_2, on = ['key'], how = 'inner')\n",
    "df_merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778fe9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---- Example_2 ---- Join with a inner (Keep values from the left size of join (df_example) otherwise will become null values)')\n",
    "df_merged = df_example.join(df_example_2, on = ['key'], how = 'left')\n",
    "df_merged.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe6378",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('---- Example_3 ----Join with a inner (Keep values from the right size of join (df_example_2) otherwise will become null values)')\n",
    "df_merged = df_example.join(df_example_2, on = ['key'], how = 'right')\n",
    "df_merged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22815331",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:28:59.888836Z",
     "iopub.status.busy": "2022-08-05T19:28:59.888167Z",
     "iopub.status.idle": "2022-08-05T19:29:00.171094Z",
     "shell.execute_reply": "2022-08-05T19:29:00.169673Z"
    },
    "papermill": {
     "duration": 0.362658,
     "end_time": "2022-08-05T19:29:00.175043",
     "exception": false,
     "start_time": "2022-08-05T19:28:59.812385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating a auxiliary df :)\n",
    "print('Creating an auxiliary Df based on df_example_2 with functions learned above')\n",
    "df_example_3 = df_example_2.withColumn('key', F.col('key')+F.lit(np.random.randint(50,100)))\n",
    "df_example_3 = df_example_3.withColumn('C5', F.concat(F.col('C5'),F.lit('_F3')))\n",
    "df_example_3 = df_example_3.withColumn('C6', F.round(F.col('C6') * F.lit(1.2)))\n",
    "df_example_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65137a4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:29:00.316282Z",
     "iopub.status.busy": "2022-08-05T19:29:00.315949Z",
     "iopub.status.idle": "2022-08-05T19:29:00.731518Z",
     "shell.execute_reply": "2022-08-05T19:29:00.730429Z"
    },
    "papermill": {
     "duration": 0.489574,
     "end_time": "2022-08-05T19:29:00.735070",
     "exception": false,
     "start_time": "2022-08-05T19:29:00.245496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('---- Example_1 ---- Concatenating Rows by Name ( same column Names)')\n",
    "df_union = df_example_2.unionByName(df_example_3)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7307aca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:29:00.923922Z",
     "iopub.status.busy": "2022-08-05T19:29:00.923479Z",
     "iopub.status.idle": "2022-08-05T19:29:01.374457Z",
     "shell.execute_reply": "2022-08-05T19:29:01.373477Z"
    },
    "papermill": {
     "duration": 0.54981,
     "end_time": "2022-08-05T19:29:01.380619",
     "exception": false,
     "start_time": "2022-08-05T19:29:00.830809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('---- Example_2 ---- Concatenating Rows ( Atention to schema of columns in both dataset, should be the same)')\n",
    "df_union = df_example_2.union(df_example_3)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57786f7",
   "metadata": {},
   "source": [
    "Understanding the difference between Union and UnionByName functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad959cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# unionByName combines two DataFrames by matching columns by name. It allows you to union DataFrames that have different column orders or even different sets of columns.\n",
    "# Sample DataFrames with different column orders and a missing column\n",
    "df_u1 = spark.createDataFrame([(1, \"Alice\", 1000)], [\"id\", \"name\", \"salary\"])\n",
    "df_u2 = spark.createDataFrame([(\"Bob\", 2)], [\"name\", \"id\"])\n",
    "\n",
    "# Union by matching columns by name\n",
    "df_union_by_name = df_u1.unionByName(df_u2, allowMissingColumns=True)\n",
    "df_union_by_name.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# union is more restrictive and will let you combine only two DataFrames that have the same number of columns and the same order of columns\n",
    "# the following will trigger an error\n",
    "df_union = df_u1.union(df_u2, allowMissingColumns=True)\n",
    "df_union.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30324df6",
   "metadata": {
    "papermill": {
     "duration": 0.067318,
     "end_time": "2022-08-05T19:29:01.529283",
     "exception": false,
     "start_time": "2022-08-05T19:29:01.461965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"10\"></a>\n",
    "# <p style=\"background-color:#00FF00;height: 60px;text-align: center;vertical-align: middle;line-height: 60px;;font-family:helvetica;color:#FFFFFF;font-size:120%;text-align:center;border-radius:12px 12px;\"> UDF</p>\n",
    "\n",
    "<p style = 'font-size:120%'>User Defined Functions or UDF (Fonctions Définies par l'Utilisateur) sont une enveloppe SQL de fonctions python. Il est donc possible de créer des fonctions python et de les appliquer aux colonnes d'un dataframe pyspark. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2724f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:29:01.663127Z",
     "iopub.status.busy": "2022-08-05T19:29:01.662819Z",
     "iopub.status.idle": "2022-08-05T19:29:02.889549Z",
     "shell.execute_reply": "2022-08-05T19:29:02.888526Z"
    },
    "papermill": {
     "duration": 1.297858,
     "end_time": "2022-08-05T19:29:02.893040",
     "exception": false,
     "start_time": "2022-08-05T19:29:01.595182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Let's say that we want to remove the F3 of column C5 of df_union previously created\n",
    "# First, create a dummy function. \n",
    "\n",
    "def remove_F3(x):\n",
    "    \"\"\"Function to split string and return first element\"\"\"\n",
    "    return x.split('_')[0]\n",
    "\n",
    "print(f'Teste String ABC_F3 -> {remove_F3(\"ABC_F3\")}\\n')\n",
    "\n",
    "# Next wrapping the functions into a udf(). The second arg is the type of the return element\n",
    "remove_F3_udf = F.udf(lambda x: remove_F3(x),StringType())\n",
    "\n",
    "#Finally, transform the C5 column\n",
    "print('--- Example_1 --- Removing the F3 string from C5 Column with a custom function')\n",
    "df_union_t = df_union.withColumn('C5_tranformed', remove_F3_udf(F.col('C5')))\n",
    "df_union_t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd2e358",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-05T19:29:03.033591Z",
     "iopub.status.busy": "2022-08-05T19:29:03.033207Z",
     "iopub.status.idle": "2022-08-05T19:29:03.684626Z",
     "shell.execute_reply": "2022-08-05T19:29:03.683378Z"
    },
    "papermill": {
     "duration": 0.726204,
     "end_time": "2022-08-05T19:29:03.688022",
     "exception": false,
     "start_time": "2022-08-05T19:29:02.961818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Let's say that we want to combine the newly C5_tranformed (string) with a C6 (Number) multiplied by a factor (Why not?)\n",
    "# First, create a dummy function. \n",
    "\n",
    "def combination_Func(st, num, factor):\n",
    "    \"\"\"Combine a string (st) with a num times a factor\"\"\"\n",
    "    return st+'_'+str(round(num*factor))\n",
    "\n",
    "print(f'Teste String ABC \\ Number 10 \\ Factor 0.2 -> {combination_Func(\"ABC_F3\",10,0.2)}')\n",
    "\n",
    "# Next wrapping the functions into a udf(). The second arg is the type of the return element\n",
    "combination_Func_udf = F.udf(lambda x,y,z: combination_Func(x,y,z),StringType())\n",
    "\n",
    "#Finally, transform the C5 column\n",
    "print('--- Example_2 --- Creating a new column C7 based on C5_tranformed and C6 and function combination_Func')\n",
    "df_union_t.withColumn('C7', combination_Func_udf(F.col('C5_tranformed'),F.col('C6'), F.lit(0.2))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65281cd8",
   "metadata": {
    "papermill": {
     "duration": 0.06617,
     "end_time": "2022-08-05T19:29:03.844088",
     "exception": false,
     "start_time": "2022-08-05T19:29:03.777918",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"20\"></a>\n",
    "# <p style=\"background-color:#00FF00;height: 60px;text-align: center;vertical-align: middle;line-height: 60px;;font-family:helvetica;color:#FFFFFF;font-size:120%;text-align:center;border-radius:12px 12px;\"> REFERENCES</p>\n",
    "\n",
    "* [Spark Apache](https://spark.apache.org/)\n",
    "* [Spark Documentation](https://spark.apache.org/docs/2.4.0/api/python/)\n",
    "* [Spark SQL Functions](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.functions)\n",
    "* (Book) Spark: The Definitive Guide: Big Data Processing Made Simple\n",
    "* (Book) Essential PySpark for Scalable Data Analytics: A beginner's guide to harnessing the power and ease of PySpark 3\n",
    "* https://www.kaggle.com/code/paulojunqueira/hands-on-pyspark-introduction-101/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab03f19f",
   "metadata": {},
   "source": [
    "<a id=\"20\"></a>\n",
    "# <p style=\"background-color:#00FF00;height: 60px;text-align: center;vertical-align: middle;line-height: 60px;;font-family:helvetica;color:#FFFFFF;font-size:120%;text-align:center;border-radius:12px 12px;\"> Cheatsheets</p>\n",
    "\n",
    "* [Pyspark Basics](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 109.073171,
   "end_time": "2022-08-05T19:29:06.799053",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-05T19:27:17.725882",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
